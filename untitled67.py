# -*- coding: utf-8 -*-
"""Untitled67.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OBsw3sMyj98KWL6-MqJd_iUFhWlPPN3g
"""

import pandas as pd
import numpy as np

df=pd.read_csv('/content/BankNote_Authentication.csv')

df.head()

df.describe()

df.info()

### Independent and Dependent features
X=df.iloc[:,:-1]
y=df.iloc[:,-1]

X.head()

y.head(20)

### Train Test Split
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)

#alpha= []
class NaiveBayes:
    
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.mean = np.zeros((len(self.classes), X.shape[1]))
        self.variance = np.zeros((len(self.classes), X.shape[1]))
        self.prior = np.zeros(len(self.classes))
        for i, c in enumerate(self.classes):
            X_c = X[y==c]
            self.mean[i, :] = X_c.mean(axis=0)
            self.variance[i, :] = X_c.var(axis=0)
            self.prior[i] = X_c.shape[0] / X.shape[0]
            
    def predict(self, X):
        posterior = np.zeros((X.shape[0], len(self.classes)))
        for i, c in enumerate(self.classes):
            likelihood = np.exp(-(X - self.mean[i, :]) ** 2 / (2 * self.variance[i, :])) / np.sqrt(2 * np.pi * self.variance[i, :])
            posterior[:, i] = np.log(likelihood).sum(axis=1) + np.log(self.prior[i])
        return self.classes[np.argmax(posterior, axis=1)]

nb=NaiveBayes()
nb.fit(X_train,y_train)

def accuracy(y_true, y_pred):
        accuracy = np.sum(y_true == y_pred) / len(y_true)
        return accuracy

nb = NaiveBayes()
nb.fit(X_train, y_train)
predictions = nb.predict(X_test)

print("Naive Bayes classification test data accuracy", accuracy(y_test, predictions)*100)

y_pred = nb.predict(X_test)

# Calculating true positives, false positives, false negatives, and true negatives
TP = sum((y_test == 1) & (y_pred == 1))
FP = sum((y_test == 0) & (y_pred == 1))
FN = sum((y_test == 1) & (y_pred == 0))
TN = sum((y_test == 0) & (y_pred == 0))

# Calculating precision, recall, and F1 score
precision = TP / (TP + FP)
recall = TP / (TP + FN)
f1_score = 2 * (precision * recall) / (precision + recall)

# Printing the precision, recall, and F1 score values
print('Precision:', precision*100)
print('Recall:', recall)
print('F1 Score:', f1_score*100)

import pickle

# main.py

import pickle

nb=NaiveBayes()

# train the model

# pickle the model
with open("nb.pkl", "wb") as f:
    pickle.dump(nb, f)